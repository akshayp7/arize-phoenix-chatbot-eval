{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2df6e-1b7f-4941-b966-8a58094570b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade \"langchain>=0.1.0\" langchain-community \"arize-phoenix[evals]\" nest-asyncio pyarrow python-docx pydantic phoenix requests pyprojroot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4f2f2d-1846-40c3-9a8f-fdf7bea3c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library ---\n",
    "import html\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from typing import Any, Dict\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# --- Core Third-Party Libraries ---\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from docx import Document\n",
    "from pydantic import BaseModel, Field\n",
    "from pyprojroot import here\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "# --- Langchain and Langchain Community ---\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import SystemMessage, HumanMessage, Document as LangchainDocument\n",
    "from langchain.utilities import SQLDatabase\n",
    "from langchain_community.agent_toolkits import create_sql_agent\n",
    "from langchain_community.utilities import SQLDatabase as CommunitySQLDatabase\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate as CoreChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.runnables import RunnableSerializable\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# --- Phoenix, OpenInference & OpenTelemetry (for tracing and evals) ---\n",
    "import phoenix as px\n",
    "from phoenix.evals import (\n",
    "    HALLUCINATION_PROMPT_RAILS_MAP,\n",
    "    HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    QA_PROMPT_RAILS_MAP,\n",
    "    QA_PROMPT_TEMPLATE,\n",
    "    TOXICITY_PROMPT_TEMPLATE,\n",
    "    HallucinationEvaluator,\n",
    "    OpenAIModel,\n",
    "    QAEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    llm_classify,\n",
    "    run_evals,\n",
    ")\n",
    "from phoenix.otel import register\n",
    "from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n",
    "from phoenix.trace import DocumentEvaluations, SpanEvaluations\n",
    "from phoenix.trace.dsl import SpanQuery\n",
    "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.instrumentation.requests import RequestsInstrumentor\n",
    "\n",
    "# --- Runtime Configuration ---\n",
    "warnings.filterwarnings('ignore')\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cc1b2c-83da-4004-8560-217e000af1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_llm() -> AzureChatOpenAI:\n",
    "    \"\"\"Initializes the AzureChatOpenAI model.\"\"\"\n",
    "    try:\n",
    "        llm = AzureChatOpenAI(\n",
    "            model=\"<your-model-name>\",\n",
    "            azure_endpoint=\"https://<your-azure-openai-endpoint>\",\n",
    "            api_version=\"<your-azure-api-version>\",\n",
    "            api_key=\"<your-azure-api-key>\",\n",
    "            azure_deployment=\"<your-azure-deployment-name>\"\n",
    "        )\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"\\n❌ Failed to initialize LLM: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ced342-0194-4b64-91a2-289fcb17cc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions_from_doc(doc_path: str) -> List[str]:\n",
    "    \"\"\"Extracts non-empty paragraphs (questions) from a Word document.\"\"\"\n",
    "    if not os.path.exists(doc_path):\n",
    "        raise FileNotFoundError(f\"Document not found: {doc_path}\")\n",
    "    try:\n",
    "        doc = Document(doc_path)\n",
    "        return [para.text.strip() for para in doc.paragraphs if para.text.strip()]\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"\\n❌ Error reading document: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb95bae4-ebe3-4173-8612-6d77a46d2c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_variations(llm_chain, questions: List[str]) -> List[str]:\n",
    "    \"\"\"Uses the LLM chain to generate question variations.\"\"\"\n",
    "    variations = []\n",
    "\n",
    "    for question in tqdm(questions, desc=\"Generating Questionnaire Variations...\", unit=\"question\"):\n",
    "        prompt_text = (\n",
    "            f\"Generate all possible positive and negative variations of the question: '{question}'. \"\n",
    "            \"Only output the questions. Do not include any explanations or answers or serial numbering or dashes.\"\n",
    "        )\n",
    "        try:\n",
    "            result = llm_chain.invoke(prompt_text)\n",
    "            content = html.unescape(result.content).encode().decode(\"unicode_escape\")\n",
    "            variations.extend(filter(None, content.split(\"\\n\")))\n",
    "            variations.append(question)  # Include original\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error processing question: '{question}': {e}\")\n",
    "\n",
    "    return list(set(variations))  # Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afedc01c-c4de-48fe-98ef-c88469dce3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Word document containing sample questions.\n",
    "# The user should keep example/sample questions in this document,\n",
    "# from which additional question variations will be generated automatically.\n",
    "doc_path = \"C:/Users/Desktop/Questionnaire.docx\"\n",
    "try:\n",
    "    # Initialize the Azure-hosted OpenAI LLM\n",
    "    llm = initialize_llm()\n",
    "    \n",
    "    # Create a chat prompt with a system and human message template\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"{text}\")\n",
    "    ])\n",
    "\n",
    "    # Create a simple prompt-to-LLM pipeline (LangChain chain)\n",
    "    chain = prompt | llm\n",
    "\n",
    "    # Extract questions from the Word document\n",
    "    questions = extract_questions_from_doc(doc_path)\n",
    "\n",
    "    # Generate variations for each question using the LLM chain\n",
    "    all_variations = generate_variations(chain, questions)\n",
    "\n",
    "    # Output the generated question variations\n",
    "    print(\"\\nGenerated Variations:\\n\" + \"-\"*50)\n",
    "    for variation in all_variations:\n",
    "        print(variation)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a95642d-5b57-4757-afc0-5f049690ce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Path to the SQLite Database ---\n",
    "\n",
    "db_path = \"C:/path/to/your/database/MySQLDatabase.db\"\n",
    "\n",
    "# --- Load the SQLite database using LangChain's SQLDatabase utility ---\n",
    "db = SQLDatabase.from_uri(f\"sqlite:///{db_path}\")\n",
    "\n",
    "# --- Custom prompt instructions to guide the LLM's SQL generation and response formatting ---\n",
    "custom_prompt_instructions = \"\"\"\n",
    "You are a SQLite expert and a data chatbot. You take in a user query, translate it into SQL, and output several pieces of metadata that help visualize the SQL output.\n",
    "\"\"\"\n",
    "\n",
    "# --- Create ChatPromptTemplate ---\n",
    "# The \"openai-tools\" agent type will automatically get tool descriptions.\n",
    "# We provide the main system instructions, the human input, and a placeholder for agent working steps.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(custom_prompt_instructions), # System context\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\"), # This is where user's question goes\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # Tracks agent's reasoning steps\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent_executor = create_sql_agent(\n",
    "    llm,                       # Previously initialized LLM\n",
    "    db=db,                     # SQLDatabase connection to SQLite\n",
    "    agent_type=\"openai-tools\", # Agent type that supports tool-use reasoning\n",
    "    verbose=True,              # Enables detailed logging/output\n",
    "    prompt=prompt              # Custom prompt template for SQL reasoning\n",
    ")\n",
    "\n",
    "# --- Store variations and responses for traceability and evaluation ---\n",
    "ref_inputs = []    # User input variations\n",
    "ref_outputs = []   # Agent-generated answers\n",
    "\n",
    "# --- Loop through the question variations and get responses from the SQL agent ---\n",
    "for variation in all_variations[:1]:\n",
    "    response = agent_executor.invoke({\"input\": variation})\n",
    "    ref_inputs.append(variation)  # input to agent\n",
    "    ref_outputs.append(response[\"output\"])  # agent's SQL-based answer\n",
    "\n",
    "# --- Create a Pandas DataFrame containing 'input' and 'reference' fields, as required by Arize Phoenix for evaluation ---\n",
    "ref_df = pd.DataFrame({\"input\": ref_inputs, \"reference\": ref_outputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcafaa0-20cd-49f7-9119-07d582d74ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the DataFrame containing inputs and reference outputs for review\n",
    "ref_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3b0a41-e809-4fa7-819e-c4ec7e793201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and register the OpenTelemetry tracer \n",
    "tracer_provider = register()\n",
    "\n",
    "# Instrument LangChain library with the tracer provider to enable automatic tracing of LangChain operations\n",
    "# skip_dep_check=True bypasses dependency checks to ensure instrumentation proceeds without interruptions\n",
    "LangChainInstrumentor(tracer_provider=tracer_provider).instrument(skip_dep_check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43976cf-a6fb-4237-aeee-e3981c64eadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch and start a Phoenix session for interactive exploration and visualization of evaluation data\n",
    "# The web app by default runs on port 6006\n",
    "session = px.launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a679350-0881-4a1e-aa26-0616d7085600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an OpenTelemetry tracer named \"langchain\"\n",
    "tracer = trace.get_tracer(\"langchain\")\n",
    "\n",
    "\n",
    "class RequestsChain(RunnableSerializable):\n",
    "    # Define the API endpoint for chatbot requests\n",
    "    endpoint: str = Field(..., description=\"API endpoint for chatbot requests\")\n",
    "    \n",
    "    @tracer.chain  # Instrument the invoke method to capture tracing data for observability\n",
    "    def invoke(self, question) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Sends a POST request with the input question, processes the response,\n",
    "        extracts answers from HTML content, and returns cleaned answer texts.\n",
    "\n",
    "        Args:\n",
    "            question (str): The user question to send to the API.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: A list of extracted answers or an error dictionary if failed.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            input = {\"ques_text\": question}\n",
    "            response = requests.post(self.endpoint, data=json.dumps(input)) # Make POST request to the configured endpoint with JSON payload\n",
    "            response.raise_for_status()  # Ensure any HTTP errors raise exceptions\n",
    "            data = response.json() # Parse JSON response from the API\n",
    "            answers = []\n",
    "            # Loop through the items under the key \"0\" in the response data\n",
    "            for item in data[\"0\"]:              \n",
    "                if item[\"type\"] == \"Text\":\n",
    "                    html = item[\"value\"]\n",
    "                    text = re.sub(r'<[^>]+>', '', html) # Strip HTML tags to get plain text\n",
    "                    match = re.search(r'Answer:\\s*(.*)', text, re.DOTALL) # Extract the part of the text after \"Answer:\" keyword\n",
    "                    if match:\n",
    "                        answer_text = match.group(1).strip() # Clean extracted answer text\n",
    "                        answers.append(answer_text)\n",
    "            return answers # Return the list of cleaned answers\n",
    "        except Exception as e:  # On failure, return error message along with the original input for debugging\n",
    "            return {\"error\": str(e), \"input\": input}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013d82d0-b3b4-41b7-ab5b-78b282405269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace <your-api-domain> and <your-api-endpoint> with your actual API base URL and path\n",
    "API_URL = \"https://<your-api-domain>/<your-api-endpoint>\" # Define the chatbot API endpoint\n",
    "chain = RequestsChain(endpoint=API_URL) # Create an instance of the RequestsChain with the specified endpoint\n",
    "chain_type = \"stuff\" # Define the type of chain being used (here \"stuff\" is a placeholder type)\n",
    "chain_metadata={\"application_type\": \"question_answering\"} # Additional metadata about the chain, useful for tracking in observability tools like Arize Phoenix\n",
    "\n",
    "# Loop through the first 5 variations and invoke the chain for each\n",
    "# tqdm is used to show a progress bar during iteration\n",
    "for variation in tqdm(all_variations):\n",
    "    chain.invoke(variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694f1a6-1c85-436f-ba4a-b3ea4d9ddb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query all CHAIN span inputs from Phoenix traces\n",
    "# Selects the span_id and the input values associated with the chain-level spans\n",
    "query = SpanQuery().where(\"span_kind == 'CHAIN'\").select(\n",
    "    span_id=\"context.span_id\", input=\"input.value\"\n",
    ")\n",
    "input_df = pd.DataFrame(px.Client().query_spans(query))\n",
    "\n",
    "# Query all CHAIN span outputs\n",
    "# Selects the span_id and the output values from the same chain-level spans\n",
    "query = SpanQuery().where(\"span_kind == 'CHAIN'\").select(\n",
    "    span_id=\"context.span_id\", output=\"output.value\"\n",
    ")\n",
    "output_df = pd.DataFrame(px.Client().query_spans(query))\n",
    "\n",
    "# Merge input and output DataFrames on span_id\n",
    "# This creates a single DataFrame that contains both input and output for each traceable span using context.span_id as the join key\n",
    "queries_df = pd.merge(input_df, output_df, on=\"context.span_id\", how=\"outer\")\n",
    "\n",
    "# Display the queries_df DataFrame combining context.span_id, inputs, outputs\n",
    "queries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db5fa1-713e-41a5-8dc3-45df3e52d6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'context.span_id' is a column in the DataFrame; reset index if it's currently the index\n",
    "if \"context.span_id\" not in queries_df.columns:\n",
    "    queries_df = queries_df.reset_index()\n",
    "\n",
    "# Merge with reference DataFrame to attach the ground truth (reference answers)\n",
    "# Arize Phoenix requires the following columns for evaluation:\n",
    "# - 'context.span_id' : unique identifier for each trace/span\n",
    "# - 'input'           : the user query or prompt\n",
    "# - 'output'          : the model's predicted response\n",
    "# - 'reference'       : the ground truth answer used for evaluation metrics\n",
    "merged_df = pd.merge(queries_df, ref_df, on=\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8bbccc-cdb1-4516-874a-b251c06d196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame containing merged data, combining context.span_id, inputs, outputs, and references for evaluation or analysis\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f19286-d8ef-4cbb-8d9c-06c5ae28bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAIModel for evaluation using Azure OpenAI deployment\n",
    "# This model will be used to compare model outputs against reference answers\n",
    "\n",
    "eval_model = OpenAIModel(\n",
    "    model=\"<your-model-name>\",\n",
    "    azure_endpoint=\"https://<your-azure-openai-endpoint>\",\n",
    "    api_version=\"<your-azure-api-version>\",\n",
    "    api_key=\"<your-azure-api-key>\",\n",
    "    azure_deployment=\"<your-azure-deployment-name>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c86db-b2b9-42bb-88ad-85b259e44f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index to 'context.span_id' for traceability during evaluations\n",
    "merged_df = merged_df.set_index(\"context.span_id\")\n",
    "\n",
    "# Perform QA Correctness evaluation using LLM classification\n",
    "qa_correctness_eval = llm_classify(\n",
    "    dataframe=merged_df,\n",
    "    model=eval_model,  # The evaluation model (e.g., GPT-4o via Azure)\n",
    "    template=QA_PROMPT_TEMPLATE,  # Prompt template for evaluating correctness\n",
    "    rails=list(QA_PROMPT_RAILS_MAP.values()),  # Expected answer categories (e.g., Correct, Incorrect)\n",
    "    provide_explanation=True,  # Ask LLM to explain its reasoning for transparency\n",
    "    concurrency=4  # Run 4 evaluations concurrently for performance\n",
    ")\n",
    "\n",
    "# Perform Hallucination evaluation using LLM classification\n",
    "hallucination_eval = llm_classify(\n",
    "    dataframe=merged_df,\n",
    "    model=eval_model,\n",
    "    template=HALLUCINATION_PROMPT_TEMPLATE,  # Prompt template for hallucination detection\n",
    "    rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),  # Expected outputs like Hallucinated, Factual\n",
    "    provide_explanation=True,\n",
    "    concurrency=4\n",
    ")\n",
    "\n",
    "# Log the evaluations back into Arize Phoenix using the Phoenix client\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval),\n",
    "    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
